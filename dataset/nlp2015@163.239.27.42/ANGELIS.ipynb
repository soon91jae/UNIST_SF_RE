{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import html\n",
    "import collections\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "angelis = pickle.load(open('ANGELIS_DATA.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_properties = {'annotators': 'tokenize, pos, ner, depparse',\n",
    "                      'pipelineLanguage':'en',\n",
    "                      'outputFormat': 'json'}\n",
    "sNLP = StanfordCoreNLP('http://localhost', port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens': [{'characterOffsetEnd': 3, 'before': '', 'originalText': 'Ali', 'ner': 'PERSON', 'word': 'Ali', 'pos': 'NNP', 'index': 1, 'after': ' ', 'lemma': 'Ali', 'characterOffsetBegin': 0}, {'characterOffsetEnd': 8, 'before': ' ', 'originalText': 'lied', 'ner': 'O', 'word': 'lied', 'pos': 'VBD', 'index': 2, 'after': ' ', 'lemma': 'lie', 'characterOffsetBegin': 4}, {'characterOffsetEnd': 14, 'before': ' ', 'originalText': 'about', 'ner': 'O', 'word': 'about', 'pos': 'IN', 'index': 3, 'after': ' ', 'lemma': 'about', 'characterOffsetBegin': 9}, {'characterOffsetEnd': 21, 'before': ' ', 'originalText': 'having', 'ner': 'O', 'word': 'having', 'pos': 'VBG', 'index': 4, 'after': ' ', 'lemma': 'have', 'characterOffsetBegin': 15}, {'characterOffsetEnd': 24, 'before': ' ', 'originalText': 'to', 'ner': 'O', 'word': 'to', 'pos': 'TO', 'index': 5, 'after': ' ', 'lemma': 'to', 'characterOffsetBegin': 22}, {'characterOffsetEnd': 30, 'before': ' ', 'originalText': 'leave', 'ner': 'O', 'word': 'leave', 'pos': 'VB', 'index': 6, 'after': ' ', 'lemma': 'leave', 'characterOffsetBegin': 25}, {'characterOffsetEnd': 34, 'before': ' ', 'originalText': 'for', 'ner': 'O', 'word': 'for', 'pos': 'IN', 'index': 7, 'after': ' ', 'lemma': 'for', 'characterOffsetBegin': 31}, {'characterOffsetEnd': 38, 'before': ' ', 'originalText': 'her', 'ner': 'O', 'word': 'her', 'pos': 'PRP$', 'index': 8, 'after': ' ', 'lemma': 'she', 'characterOffsetBegin': 35}, {'characterOffsetEnd': 42, 'before': ' ', 'originalText': 'job', 'ner': 'O', 'word': 'job', 'pos': 'NN', 'index': 9, 'after': ' ', 'lemma': 'job', 'characterOffsetBegin': 39}, {'characterOffsetEnd': 45, 'before': ' ', 'originalText': 'to', 'ner': 'O', 'word': 'to', 'pos': 'TO', 'index': 10, 'after': ' ', 'lemma': 'to', 'characterOffsetBegin': 43}, {'characterOffsetEnd': 49, 'before': ' ', 'originalText': 'see', 'ner': 'O', 'word': 'see', 'pos': 'VB', 'index': 11, 'after': ' ', 'lemma': 'see', 'characterOffsetBegin': 46}, {'characterOffsetEnd': 52, 'before': ' ', 'originalText': 'if', 'ner': 'O', 'word': 'if', 'pos': 'IN', 'index': 12, 'after': ' ', 'lemma': 'if', 'characterOffsetBegin': 50}, {'characterOffsetEnd': 57, 'before': ' ', 'originalText': 'Jake', 'ner': 'PERSON', 'word': 'Jake', 'pos': 'NNP', 'index': 13, 'after': ' ', 'lemma': 'Jake', 'characterOffsetBegin': 53}, {'characterOffsetEnd': 63, 'before': ' ', 'originalText': 'would', 'ner': 'O', 'word': 'would', 'pos': 'MD', 'index': 14, 'after': ' ', 'lemma': 'would', 'characterOffsetBegin': 58}, {'characterOffsetEnd': 67, 'before': ' ', 'originalText': 'end', 'ner': 'O', 'word': 'end', 'pos': 'VB', 'index': 15, 'after': ' ', 'lemma': 'end', 'characterOffsetBegin': 64}, {'characterOffsetEnd': 71, 'before': ' ', 'originalText': 'the', 'ner': 'O', 'word': 'the', 'pos': 'DT', 'index': 16, 'after': ' ', 'lemma': 'the', 'characterOffsetBegin': 68}, {'characterOffsetEnd': 76, 'before': ' ', 'originalText': 'show', 'ner': 'O', 'word': 'show', 'pos': 'NN', 'index': 17, 'after': ' ', 'lemma': 'show', 'characterOffsetBegin': 72}, {'characterOffsetEnd': 79, 'before': ' ', 'originalText': 'to', 'ner': 'O', 'word': 'to', 'pos': 'TO', 'index': 18, 'after': ' ', 'lemma': 'to', 'characterOffsetBegin': 77}, {'characterOffsetEnd': 82, 'before': ' ', 'originalText': 'be', 'ner': 'O', 'word': 'be', 'pos': 'VB', 'index': 19, 'after': ' ', 'lemma': 'be', 'characterOffsetBegin': 80}, {'characterOffsetEnd': 87, 'before': ' ', 'originalText': 'with', 'ner': 'O', 'word': 'with', 'pos': 'IN', 'index': 20, 'after': ' ', 'lemma': 'with', 'characterOffsetBegin': 83}, {'characterOffsetEnd': 91, 'before': ' ', 'originalText': 'her', 'ner': 'O', 'word': 'her', 'pos': 'PRP', 'index': 21, 'after': ' ', 'lemma': 'she', 'characterOffsetBegin': 88}, {'characterOffsetEnd': 93, 'before': ' ', 'originalText': '.', 'ner': 'O', 'word': '.', 'pos': '.', 'index': 22, 'after': '', 'lemma': '.', 'characterOffsetBegin': 92}], 'basicDependencies': [{'governor': 0, 'dependentGloss': 'lied', 'governorGloss': 'ROOT', 'dep': 'ROOT', 'dependent': 2}, {'governor': 2, 'dependentGloss': 'Ali', 'governorGloss': 'lied', 'dep': 'nsubj', 'dependent': 1}, {'governor': 4, 'dependentGloss': 'about', 'governorGloss': 'having', 'dep': 'mark', 'dependent': 3}, {'governor': 2, 'dependentGloss': 'having', 'governorGloss': 'lied', 'dep': 'advcl', 'dependent': 4}, {'governor': 6, 'dependentGloss': 'to', 'governorGloss': 'leave', 'dep': 'mark', 'dependent': 5}, {'governor': 4, 'dependentGloss': 'leave', 'governorGloss': 'having', 'dep': 'xcomp', 'dependent': 6}, {'governor': 11, 'dependentGloss': 'for', 'governorGloss': 'see', 'dep': 'mark', 'dependent': 7}, {'governor': 9, 'dependentGloss': 'her', 'governorGloss': 'job', 'dep': 'nmod:poss', 'dependent': 8}, {'governor': 11, 'dependentGloss': 'job', 'governorGloss': 'see', 'dep': 'nsubj', 'dependent': 9}, {'governor': 11, 'dependentGloss': 'to', 'governorGloss': 'see', 'dep': 'mark', 'dependent': 10}, {'governor': 6, 'dependentGloss': 'see', 'governorGloss': 'leave', 'dep': 'advcl', 'dependent': 11}, {'governor': 15, 'dependentGloss': 'if', 'governorGloss': 'end', 'dep': 'mark', 'dependent': 12}, {'governor': 15, 'dependentGloss': 'Jake', 'governorGloss': 'end', 'dep': 'nsubj', 'dependent': 13}, {'governor': 15, 'dependentGloss': 'would', 'governorGloss': 'end', 'dep': 'aux', 'dependent': 14}, {'governor': 11, 'dependentGloss': 'end', 'governorGloss': 'see', 'dep': 'advcl', 'dependent': 15}, {'governor': 17, 'dependentGloss': 'the', 'governorGloss': 'show', 'dep': 'det', 'dependent': 16}, {'governor': 15, 'dependentGloss': 'show', 'governorGloss': 'end', 'dep': 'dobj', 'dependent': 17}, {'governor': 21, 'dependentGloss': 'to', 'governorGloss': 'her', 'dep': 'mark', 'dependent': 18}, {'governor': 21, 'dependentGloss': 'be', 'governorGloss': 'her', 'dep': 'cop', 'dependent': 19}, {'governor': 21, 'dependentGloss': 'with', 'governorGloss': 'her', 'dep': 'case', 'dependent': 20}, {'governor': 17, 'dependentGloss': 'her', 'governorGloss': 'show', 'dep': 'acl', 'dependent': 21}, {'governor': 2, 'dependentGloss': '.', 'governorGloss': 'lied', 'dep': 'punct', 'dependent': 22}], 'enhancedDependencies': [{'governor': 0, 'dependentGloss': 'lied', 'governorGloss': 'ROOT', 'dep': 'ROOT', 'dependent': 2}, {'governor': 2, 'dependentGloss': 'Ali', 'governorGloss': 'lied', 'dep': 'nsubj', 'dependent': 1}, {'governor': 4, 'dependentGloss': 'about', 'governorGloss': 'having', 'dep': 'mark', 'dependent': 3}, {'governor': 2, 'dependentGloss': 'having', 'governorGloss': 'lied', 'dep': 'advcl:about', 'dependent': 4}, {'governor': 6, 'dependentGloss': 'to', 'governorGloss': 'leave', 'dep': 'mark', 'dependent': 5}, {'governor': 4, 'dependentGloss': 'leave', 'governorGloss': 'having', 'dep': 'xcomp', 'dependent': 6}, {'governor': 11, 'dependentGloss': 'for', 'governorGloss': 'see', 'dep': 'mark', 'dependent': 7}, {'governor': 9, 'dependentGloss': 'her', 'governorGloss': 'job', 'dep': 'nmod:poss', 'dependent': 8}, {'governor': 11, 'dependentGloss': 'job', 'governorGloss': 'see', 'dep': 'nsubj', 'dependent': 9}, {'governor': 11, 'dependentGloss': 'to', 'governorGloss': 'see', 'dep': 'mark', 'dependent': 10}, {'governor': 6, 'dependentGloss': 'see', 'governorGloss': 'leave', 'dep': 'advcl:to', 'dependent': 11}, {'governor': 15, 'dependentGloss': 'if', 'governorGloss': 'end', 'dep': 'mark', 'dependent': 12}, {'governor': 15, 'dependentGloss': 'Jake', 'governorGloss': 'end', 'dep': 'nsubj', 'dependent': 13}, {'governor': 15, 'dependentGloss': 'would', 'governorGloss': 'end', 'dep': 'aux', 'dependent': 14}, {'governor': 11, 'dependentGloss': 'end', 'governorGloss': 'see', 'dep': 'advcl:if', 'dependent': 15}, {'governor': 17, 'dependentGloss': 'the', 'governorGloss': 'show', 'dep': 'det', 'dependent': 16}, {'governor': 15, 'dependentGloss': 'show', 'governorGloss': 'end', 'dep': 'dobj', 'dependent': 17}, {'governor': 21, 'dependentGloss': 'to', 'governorGloss': 'her', 'dep': 'mark', 'dependent': 18}, {'governor': 21, 'dependentGloss': 'be', 'governorGloss': 'her', 'dep': 'cop', 'dependent': 19}, {'governor': 21, 'dependentGloss': 'with', 'governorGloss': 'her', 'dep': 'case', 'dependent': 20}, {'governor': 17, 'dependentGloss': 'her', 'governorGloss': 'show', 'dep': 'acl:with', 'dependent': 21}, {'governor': 2, 'dependentGloss': '.', 'governorGloss': 'lied', 'dep': 'punct', 'dependent': 22}], 'enhancedPlusPlusDependencies': [{'governor': 0, 'dependentGloss': 'lied', 'governorGloss': 'ROOT', 'dep': 'ROOT', 'dependent': 2}, {'governor': 2, 'dependentGloss': 'Ali', 'governorGloss': 'lied', 'dep': 'nsubj', 'dependent': 1}, {'governor': 4, 'dependentGloss': 'about', 'governorGloss': 'having', 'dep': 'mark', 'dependent': 3}, {'governor': 2, 'dependentGloss': 'having', 'governorGloss': 'lied', 'dep': 'advcl:about', 'dependent': 4}, {'governor': 6, 'dependentGloss': 'to', 'governorGloss': 'leave', 'dep': 'mark', 'dependent': 5}, {'governor': 4, 'dependentGloss': 'leave', 'governorGloss': 'having', 'dep': 'xcomp', 'dependent': 6}, {'governor': 11, 'dependentGloss': 'for', 'governorGloss': 'see', 'dep': 'mark', 'dependent': 7}, {'governor': 9, 'dependentGloss': 'her', 'governorGloss': 'job', 'dep': 'nmod:poss', 'dependent': 8}, {'governor': 11, 'dependentGloss': 'job', 'governorGloss': 'see', 'dep': 'nsubj', 'dependent': 9}, {'governor': 11, 'dependentGloss': 'to', 'governorGloss': 'see', 'dep': 'mark', 'dependent': 10}, {'governor': 6, 'dependentGloss': 'see', 'governorGloss': 'leave', 'dep': 'advcl:to', 'dependent': 11}, {'governor': 15, 'dependentGloss': 'if', 'governorGloss': 'end', 'dep': 'mark', 'dependent': 12}, {'governor': 15, 'dependentGloss': 'Jake', 'governorGloss': 'end', 'dep': 'nsubj', 'dependent': 13}, {'governor': 15, 'dependentGloss': 'would', 'governorGloss': 'end', 'dep': 'aux', 'dependent': 14}, {'governor': 11, 'dependentGloss': 'end', 'governorGloss': 'see', 'dep': 'advcl:if', 'dependent': 15}, {'governor': 17, 'dependentGloss': 'the', 'governorGloss': 'show', 'dep': 'det', 'dependent': 16}, {'governor': 15, 'dependentGloss': 'show', 'governorGloss': 'end', 'dep': 'dobj', 'dependent': 17}, {'governor': 21, 'dependentGloss': 'to', 'governorGloss': 'her', 'dep': 'mark', 'dependent': 18}, {'governor': 21, 'dependentGloss': 'be', 'governorGloss': 'her', 'dep': 'cop', 'dependent': 19}, {'governor': 21, 'dependentGloss': 'with', 'governorGloss': 'her', 'dep': 'case', 'dependent': 20}, {'governor': 17, 'dependentGloss': 'her', 'governorGloss': 'show', 'dep': 'acl:with', 'dependent': 21}, {'governor': 2, 'dependentGloss': '.', 'governorGloss': 'lied', 'dep': 'punct', 'dependent': 22}], 'index': 0, 'entitymentions': [{'characterOffsetEnd': 3, 'tokenBegin': 0, 'docTokenBegin': 0, 'ner': 'PERSON', 'text': 'Ali', 'docTokenEnd': 1, 'tokenEnd': 1, 'characterOffsetBegin': 0}, {'characterOffsetEnd': 57, 'tokenBegin': 12, 'docTokenBegin': 12, 'ner': 'PERSON', 'text': 'Jake', 'docTokenEnd': 13, 'tokenEnd': 13, 'characterOffsetBegin': 53}, {'characterOffsetEnd': 38, 'tokenBegin': 7, 'docTokenBegin': 7, 'ner': 'PERSON', 'text': 'her', 'docTokenEnd': 8, 'tokenEnd': 8, 'characterOffsetBegin': 35}, {'characterOffsetEnd': 91, 'tokenBegin': 20, 'docTokenBegin': 20, 'ner': 'PERSON', 'text': 'her', 'docTokenEnd': 21, 'tokenEnd': 21, 'characterOffsetBegin': 88}]}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fa03b3c86f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdeprel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdeptoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'basicDependencies'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "parsed = sNLP.annotate('Ali lied about having to leave for her job to see if Jake would end the show to be with her .', properties = default_properties)\n",
    "print(json.loads(parsed)['sentences'])\n",
    "tokens = []\n",
    "head = [0]*len(parsed['tokens'])\n",
    "deprel = [0]*len(parsed['tokens'])\n",
    "for deptoken in parsed['basicDependencies']:\n",
    "    #deptoken['dependent'] <- linear index\n",
    "    #deptoken['governor'] <- index of governor (head)\n",
    "    #deptoken['dep'] <- dep_rel\n",
    "    \n",
    "    deprel[deptoken['dependent'] - 1] = deptoken['dep']\n",
    "    head[deptoken['dependent'] - 1] = deptoken['governor']\n",
    "print(head)\n",
    "print(deprel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angelis[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(sents):\n",
    "    dataset = []\n",
    "    ners = {}\n",
    "    for sentence_index, sent in enumerate(sents):\n",
    "        print(sentence_index + 1)\n",
    "        data = {}\n",
    "        id = None\n",
    "        \n",
    "        \n",
    "        sentence = html.unescape(sent[0].strip())\n",
    "        Obj = html.unescape(sent[1][1].replace('\\n',' ').strip())\n",
    "        Subj = html.unescape(sent[1][2].replace('\\n',' ').strip())\n",
    "        \n",
    "        relation = 'NA' if sent[1][0] == 0 or sent[2]=='no_relation' else sent[2]\n",
    "        \n",
    "        tokens = []\n",
    "        ner = []\n",
    "        pos = []\n",
    "        \n",
    "        try:\n",
    "            annotated = json.loads(sNLP.annotate(sentence, properties = default_properties))\n",
    "        except:\n",
    "            continue\n",
    "        #print(annotated['sentences'][0]['tokens'])\n",
    "        for sentence in annotated['sentences']:\n",
    "            for analyzed_token in sentence['tokens']:\n",
    "                tokens.append(analyzed_token['word'])\n",
    "                ner.append(analyzed_token['ner'])\n",
    "                pos.append(analyzed_token['pos'])\n",
    "        for ne in ner:\n",
    "            if ne not in ners:\n",
    "                ners[ne] = 1\n",
    "            else:\n",
    "                ners[ne] += 1\n",
    "        tokens_str = \" \".join(tokens)\n",
    "        \n",
    "        \n",
    "        deprel = [0] * len(tokens)\n",
    "        head = [0 ] * len(tokens)\n",
    "        base_index = 0\n",
    "        for sentence in annotated['sentences']:\n",
    "            for deptoken in sentence['basicDependencies']:\n",
    "                token_index = deptoken['dependent'] - 1 + base_index\n",
    "                governor_index = 0 if deptoken['governor'] == 0 else deptoken['governor'] + base_index\n",
    "                \n",
    "                deprel[token_index] = deptoken['dep']\n",
    "                head[token_index] = governor_index\n",
    "            base_index += len(sentence['tokens'])\n",
    "                \n",
    "        Obj_tokens = []\n",
    "        Obj_ner = []\n",
    "        Subj_tokens = []\n",
    "        Subj_ner = []\n",
    "        \n",
    "        try:\n",
    "            parsed_Obj = json.loads(sNLP.annotate(Obj, properties = default_properties))\n",
    "            parsed_Subj = json.loads(sNLP.annotate(Subj, properties = default_properties))\n",
    "        except:\n",
    "            continue\n",
    "        for sent in parsed_Obj['sentences']:\n",
    "            for analyzed_token in sent['tokens']:\n",
    "                Obj_tokens.append(analyzed_token['word'])\n",
    "                Obj_ner.append(analyzed_token['ner'])\n",
    "        Obj_str = \" \".join(Obj_tokens)\n",
    "        \n",
    "        for sent in parsed_Subj['sentences']:\n",
    "            for analyzed_token in sent['tokens']:\n",
    "                Subj_tokens.append(analyzed_token['word'])\n",
    "                Subj_ner.append(analyzed_token['ner'])\n",
    "        Subj_str = \" \".join(Subj_tokens)\n",
    "        \n",
    "        print(\"\\'%s\\' \\nObj \\'%s\\' Subj \\'%s\\'\"%(tokens_str, Obj_str, Subj_str))\n",
    "        if Obj_str not in tokens_str:\n",
    "            print(\"Obj \\'%s\\' not in \\'%s\\'\"%(Obj_str, tokens_str))\n",
    "            continue\n",
    "        elif Subj_str not in tokens_str:\n",
    "            print(\"Subj \\'%s\\' not in \\'%s\\'\"%(Subj_str, tokens_str))\n",
    "            continue\n",
    "        \n",
    "        Obj_start = -1\n",
    "        Obj_end = -1\n",
    "        for i in range(len(tokens)):\n",
    "            check_flag = True\n",
    "            for j in range(len(Obj_tokens)):\n",
    "                #print(tokens[i+j])\n",
    "                if tokens[i+j] != Obj_tokens[j]:\n",
    "                    check_flag = False\n",
    "            if check_flag:\n",
    "                Obj_start = i\n",
    "                Obj_end = Obj_start + len(Obj_tokens) - 1\n",
    "                break\n",
    "        ner_counter = collections.Counter(Obj_ner)\n",
    "        Obj_type = sorted(ner_counter, key=ner_counter.get, reverse=True)[0]\n",
    "\n",
    "        Subj_start = -1\n",
    "        Subj_end = -1\n",
    "        for i in range(len(tokens)):\n",
    "            check_flag = True\n",
    "            for j in range(len(Subj_tokens)):\n",
    "                if tokens[i+j] != Subj_tokens[j]:\n",
    "                    check_flag = False\n",
    "            if check_flag:\n",
    "                Subj_start = i\n",
    "                Subj_end = Subj_start + len(Subj_tokens) - 1\n",
    "                break\n",
    "        ner_counter = collections.Counter(Subj_ner)\n",
    "        Subj_type = sorted(ner_counter, key=ner_counter.get, reverse=True)[0]\n",
    "\n",
    "        #print(\"Obj \\'%s\\' Subj \\'%s\\'\"%(\" \".join(tokens[Obj_start:Obj_end+1]), \" \".join(tokens[Subj_start:Subj_end+1])))\n",
    "        #print(\"Obj type \\'%s\\' Subj type \\'%s\\'\"%(Obj_type, Subj_type))\n",
    "        \n",
    "        \n",
    "        \n",
    "        data['id'] = id\n",
    "        data['tokens'] = tokens\n",
    "        data['sentence'] = tokens_str\n",
    "        data['ner'] = ner\n",
    "        data['pos'] = pos\n",
    "        data['head'] = head\n",
    "        data['deprel'] = deprel\n",
    "        data['key_start'] = Obj_start\n",
    "        data['key_end'] = Obj_end\n",
    "        data['key_type'] = Obj_type\n",
    "        data['ans_start'] = Subj_start\n",
    "        data['ans_end'] = Subj_end\n",
    "        data['ans_type'] = Subj_type\n",
    "        data['relation'] = 'NA' if relation == 'no_relation' else relation\n",
    "        \n",
    "        dataset.append(data)\n",
    "    print(ners)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "'Althouse College of Education is a teacher 's college at University of Western Ontario , London , Ontario , Canada .' \n",
      "Obj 'University of Western Ontario' Subj 'London'\n",
      "2\n",
      "'Moulton worked for two years as an instructor at Peter Kump 's New York Cooking School -LRB- now known as the Institute of Culinary Education -RRB- , where she discovered her love of teaching .' \n",
      "Obj 'Institute of Culinary Education' Subj 'New York'\n",
      "3\n",
      "'Norsk Transport retained for a period maintenance of the Rjukan Line , while the Tinnos Line remained part of the rail network maintained by the Norwegian National Rail Administration .' \n",
      "Obj 'Norsk Transport' Subj 'National Rail Administration'\n",
      "4\n",
      "'With her two albums , '' Taylor Swift '' in 2006 and '' Fearless '' in 2008 -LRB- both on the Big Machine label -RRB- , Swift has built her career by way of Nashville and country radio .' \n",
      "Obj 'Taylor Swift' Subj 'Big Machine'\n",
      "5\n",
      "''' Joyce ready for great leap at Qantas '' , Sydney Morning Herald online retrieved 27 November 2009 .' \n",
      "Obj 'Qantas' Subj 'Sydney'\n",
      "6\n",
      "'MySpace is a subsidiary of Beverly Hills , California-based Fox Interactive Media Inc. , which is owned by News Corp .' \n",
      "Obj 'Fox Interactive Media' Subj 'Beverly Hills'\n",
      "7\n",
      "'Abdoulaye Sékou Sow -LRB- 1931 -- 27 May 2013 -RRB- was a Malian politician who served as Prime Minister of Mali from 12 April 1993 to 4 February 1994 under President Alpha Oumar Konaré .' \n",
      "Obj 'Alpha Oumar Konaré' Subj 'Mali'\n",
      "8\n",
      "'Swedish aerospace and defense company Saab AB said Friday it has identified more than 150 potential collaboration projects if the company wins the bidding war to replace Norway 's 48 aging F-16 fighters .' \n",
      "Obj 'Saab' Subj 'war'\n",
      "9\n",
      "'To afford Kartik 's high fees , Meghna shoots a lingerie advertisement while another aspiring model , Janet -LRB- Mugdha Godse -RRB- , mentors her .' \n",
      "Obj 'Mugdha Godse' Subj 'model'\n",
      "10\n",
      "'1976 saw the release of The Best of Two Worlds , a reunion with Stan Getz , featuring singer Miúcha , -LRB- sister of Chico Buarque -RRB- , who had become Gilberto 's second wife in April 1965 .' \n",
      "Obj 'Gilberto' Subj 'singer'\n",
      "11\n",
      "'Lebanese Armed Forces -LRB- LAF -RRB- commander General Jean Kahwaji discussed Monday with United Nations Interim Force in Lebanon -LRB- UNIFIL -RRB- commander General Alberto Asarta Cuevas ways to increase coordination and cooperation between the LAF and UNIFIL .' \n",
      "Obj 'Lebanese Armed Forces' Subj 'Lebanon'\n",
      "12\n",
      "'Mark Zuckerberg of Facebook is not stepping aside for a chief executive as Larry Page and Sergey Brin did at Google or as Jerry Yang and David Filo did at Yahoo .' \n",
      "Obj 'Google' Subj 'Sergey Brin'\n",
      "13\n",
      "'Jackson was born in Vancouver , British Columbia to John Carter Jackson and his wife , Fiona .' \n",
      "Obj 'Jackson' Subj 'Vancouver'\n",
      "14\n",
      "'You already know he commanded the Continental Army in the American Revolution , was our first president and had false teeth .' \n",
      "Obj 'he' Subj 'Continental Army'\n",
      "15\n",
      "'Marc Handley Andrus is the Eighth Bishop of California in The Episcopal Church .' \n",
      "Obj 'Marc Handley Andrus' Subj 'California'\n",
      "16\n",
      "'She was in Moscow to witness the power struggles in the Soviet Communist party as Mikhail Gorbachev tried to introduce reform , reported on the break-up of the Soviet Union and the internal conflicts in Chechnya , Georgia and Tajikistan .' \n",
      "Obj 'Mikhail Gorbachev' Subj 'Soviet Union'\n",
      "17\n",
      "18\n",
      "'The 2012 -- 2013 Bryant Bulldogs men 's basketball team represented Bryant University during the 2012 -- 13 NCAA Division I men 's basketball season .' \n",
      "Obj 'Bryant Bulldogs' Subj 'Division I'\n",
      "19\n",
      "'Evagoras Pallikarides -LRB- , 26 February 1938 -- 14 March 1957 -RRB- was a member of EOKA during the 1955 -- 1959 campaign against British rule in Cyprus .' \n",
      "Obj 'EOKA' Subj 'Cyprus'\n",
      "20\n",
      "'The party was founded on 10 December 1994 by former member parties of the anti-Liberal Democratic Party -LRB- LDP -RRB- coalition led by Morihiro Hosokawa who had resigned in April .' \n",
      "Obj 'Democratic Party' Subj '1994'\n",
      "21\n",
      "'Shortly after the auction , the planned merger between Gazprom and Rosneft merger was called off , and Sergey Bogdanchikov resigned his post as CEO of Gazpromneft .' \n",
      "Obj 'Rosneft' Subj 'Sergey Bogdanchikov'\n",
      "22\n",
      "'Leader of Georgia 's breakaway region of Abkhazia Sergei Bagapsh on Wednesday ordered the navy to destroy Georgian ships said to have illegally crossed the border .' \n",
      "Obj 'Sergei Bagapsh' Subj 'Abkhazia'\n",
      "23\n",
      "'Born in Barnwell , South Carolina , Brown moved to Augusta , Georgia , to live with relatives at the age of four .' \n",
      "Obj 'Brown' Subj 'Georgia'\n",
      "24\n",
      "'Cuomo 's investigation will help hold rating services accountable '' even though it is directed against the investment banks and asking whether they have misled the ratings agencies , '' Connecticut Attorney General Richard Blumenthal said Thursday on conference call coordinated by the White House about Sen. Christopher Dodd 's financial overhaul bill .' \n",
      "Obj 'Christopher Dodd' Subj 'Connecticut'\n",
      "25\n",
      "'Shares of Issaquah-based Costco closed at $ 53.71 on the Nasdaq Stock Market , down less than 1 percent .' \n",
      "Obj 'Costco' Subj 'Issaquah-based'\n",
      "26\n",
      "'The following day the country 's Prosecutor General ordered the detention of Mubarak and his two children , Alaa Mubarak and Gamal Mubarak , for 15 days .' \n",
      "Obj 'Mubarak' Subj 'Gamal Mubarak'\n",
      "27\n",
      "'The Ministry of National Defense -LRB- Wizarat al Difaa ' al Watani -RRB- is Lebanon 's service section for the Lebanese Armed Forces which directs the entire Army .' \n",
      "Obj 'Lebanese Armed Forces' Subj 'Lebanon'\n",
      "28\n",
      "'On March 7 , 1995 , a year after BN came to power in Sabah , Musa became the director of Sabah Foundation -LRB- Yayasan Sabah -RRB- , a Sabah statutory body .' \n",
      "Obj 'Musa' Subj 'Sabah'\n",
      "29\n",
      "'Previous kidnappings acknowledged by the MNJ in the conflict -- those of a Chinese mining executive in 2007 , a Nigerien parliamentarian and Red Cross head , a Nigerien Prefect , and four Areva officials , all in 2008 -- were all quickly resolved .' \n",
      "Obj 'MNJ' Subj '2007'\n",
      "30\n",
      "'But I 'd rather take FOX 's '' propaganda '' than MSNBC 's '' whistleblowing '' any day .' \n",
      "Obj 'MSNBC' Subj 'I'\n",
      "31\n",
      "'In 1993 he was named Houghton College 's Alumnus of the Year , and received an honorary degree of Doctor of Pedagogy -LRB- DPd -RRB- from Houghton in 2000 .' \n",
      "Obj 'Houghton College' Subj 'Houghton'\n",
      "32\n",
      "'Connecticut Transit Hartford -LRB- CT Transit Hartford Division -RRB- is the largest division of Connecticut Transit , providing service on 43 local routes , 5 '' flyer '' limited stop routes and 12 express routes throughout 27 towns in Hartford County , including Bloomfield , East Hartford , Farmington , Glastonbury , Manchester , Newington , New Britain , South Windsor , West Hartford , Wethersfield and Windsor , in addition to Hartford .' \n",
      "Obj 'Connecticut Transit' Subj 'Hartford'\n",
      "33\n",
      "'Brigitte Fontaine , born in 1939 in Morlaix in the Brittany region of France , is a singer of avant-garde music .' \n",
      "Obj 'Fontaine' Subj 'Brittany'\n",
      "34\n",
      "'Viva Pink , or the Finch-Richmond Hill-Unionville line , is a bus rapid transit line in York Region , north of Toronto , Canada .' \n",
      "Obj 'Viva' Subj 'York Region'\n",
      "35\n",
      "'It was inaugurated on June 5 , 2006 replacing Rede 21 in partnership with Grupo Bandeirantes de Comunicação .' \n",
      "Obj 'Rede 21' Subj 'Grupo Bandeirantes de Comunicação'\n",
      "36\n",
      "'The New Zealand Herald is a daily newspaper published in Auckland , New Zealand , owned by APN News & Media .' \n",
      "Obj 'The New Zealand Herald' Subj 'New Zealand'\n",
      "37\n",
      "'All the Six Nations heavyweights faced elimination in their final pool matches but England and France then eliminated Australia and the All Blacks respectively in their quarter-finals .' \n",
      "Obj 'Nations' Subj 'Six'\n",
      "38\n",
      "39\n",
      "'In a statement released on the airBaltic website Tuesday , Flick said '' ... a ruling in Lithuania can not have legal effect in Latvia . ''' \n",
      "Obj 'airBaltic' Subj 'Lithuania'\n",
      "40\n",
      "41\n",
      "'Andrew is featured in the DVD Norah Jones & the Handsome Band : Live in 2004 Today he lives in Brooklyn , NY and plays with Norah Jones , Jesse Harris , and the Take-out Kings , amongst others .' \n",
      "Obj 'Norah Jones' Subj 'Brooklyn'\n",
      "42\n",
      "43\n",
      "'The 1908 Alabama Crimson Tide football team -LRB- variously '' Alabama '' , '' UA '' or '' Bama '' -RRB- represented the University of Alabama in the 1908 college football season .' \n",
      "Obj 'University of Alabama' Subj 'Crimson Tide'\n",
      "44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Cheltenham College is a co-educational independent school , located in Cheltenham , Gloucestershire , England .' \n",
      "Obj 'Cheltenham College' Subj 'England'\n",
      "45\n",
      "'In 1970 Hastings Banda was declared President for life of the MCP , and in 1971 Banda consolidated his power and was named President for life of Malawi itself .' \n",
      "Obj 'Hastings Banda' Subj 'Malawi'\n",
      "46\n",
      "47\n",
      "'Maximum Pro Wrestling -LRB- MaxPro -RRB- is a Canadian independent professional wrestling organization , founded in 2010 with the merger of Scott D'Amore 's Border City Wrestling out of Windsor , Ontario and BSE Pro Wrestling out of Toronto , Ontario .' \n",
      "Obj 'Scott D'Amore' Subj 'Windsor'\n",
      "48\n",
      "49\n",
      "'Legacy of Kain is a series of action-adventure video games primarily developed by Crystal Dynamics and published by Square Enix Europe -LRB- formerly Eidos Interactive -RRB- .' \n",
      "Obj 'Crystal Dynamics' Subj 'Europe'\n",
      "50\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "dataset = data_transform(angelis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset, open('Angelis.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(sNLP.annotate(html.unescape(angelis[1998][0]), properties = default_properties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.unescape(angelis[1998][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
